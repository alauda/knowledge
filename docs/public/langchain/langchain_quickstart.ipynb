{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Quick Start Demo\n",
    "\n",
    "This notebook demonstrates how to create a simple LangChain agent with tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"langchain>=1.0.0\" \"langchain-openai>=1.0.0\" \"requests\" \"fastapi\" \"uvicorn\" --target ~/packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "user_site_packages = Path.home() / \"packages\"\n",
    "if str(user_site_packages) not in sys.path:\n",
    "    sys.path.insert(0, str(user_site_packages))\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from typing import Dict, Any\n",
    "import requests\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Configuration\n",
    "\n",
    "Set your API key and model configuration (optional, can also use environment variables):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key by the OPENAI_API_KEY environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"<Your API Key>\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"deepseek-chat\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\", \"https://api.deepseek.com\")\n",
    "\n",
    "print(\"Environ configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a Tool\n",
    "\n",
    "Create a simple weather tool using the `@tool` decorator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(city: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get current weather for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: City name (e.g., Beijing, Shanghai, New York)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with weather information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://wttr.in/{city}?format=j1\"\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        current = data[\"current_condition\"][0]\n",
    "        return {\n",
    "            \"city\": city,\n",
    "            \"temperature\": f\"{current['temp_C']}°C\",\n",
    "            \"humidity\": f\"{current['humidity']}%\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to get weather: {str(e)}\"}\n",
    "\n",
    "print(\"Defined get_weather tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create LLM Model\n",
    "\n",
    "Initialize the language model (using DeepSeek in this example):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=BASE_URL if BASE_URL else None,  # None means use OpenAI default\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Created LLM instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Agent\n",
    "\n",
    "Create an agent with the weather tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful weather assistant.\"\n",
    ")\n",
    "\n",
    "print(\"Created Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Agent\n",
    "\n",
    "Invoke the agent with a query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.ainvoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather in Beijing?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Invoked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display the Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result: Dict[str, Any]):\n",
    "    if isinstance(result, dict) and \"messages\" in result:\n",
    "        last_message = result[\"messages\"][-1]\n",
    "        if hasattr(last_message, \"content\"):\n",
    "            print(last_message.content)\n",
    "        else:\n",
    "            print(result)\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Queries\n",
    "\n",
    "You can try asking different questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query\n",
    "result2 = await agent.ainvoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather in Shanghai?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print_result(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. FastAPI Service Example\n",
    "\n",
    "You can also run the agent as a FastAPI web service for production use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FastAPI components\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from threading import Thread\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Create a simple FastAPI app\n",
    "api_app = FastAPI(title=\"LangChain Agent API\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@api_app.post(\"/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\" Chat endpoint \"\"\"\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": request.message}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    if isinstance(result, dict) and \"messages\" in result:\n",
    "        last_message = result[\"messages\"][-1]\n",
    "        if hasattr(last_message, \"content\"):\n",
    "            return {\"response\": last_message.content}\n",
    "    return {\"response\": str(result)}\n",
    "\n",
    "\n",
    "print(\"FastAPI app created. Use the next cell to start the server.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the FastAPI Server\n",
    "\n",
    "**Note**: In a notebook, you can start the server in a background thread. For production, run it as a separate process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start server in background thread (for notebook demonstration)\n",
    "\n",
    "from uvicorn import Config, Server\n",
    "\n",
    "# Create a server instance that can be controlled\n",
    "config = Config(api_app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "server = Server(config)\n",
    "\n",
    "def run_server():\n",
    "    server.run()\n",
    "\n",
    "# Use daemon=True so the thread stops automatically when the kernel restarts\n",
    "# This is safe for notebook demonstrations\n",
    "# For production, use process managers instead of threads\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait a moment for the server to start\n",
    "time.sleep(2)\n",
    "print(\"✓ FastAPI server started at http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API\n",
    "\n",
    "Now you can call the API using HTTP requests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API endpoint\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8000/chat\",\n",
    "    json={\"message\": \"What's the weather in Shanghai?\"},\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(\"Response:\")\n",
    "print(response.json().get('response'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the Server\n",
    "\n",
    "You can stop the server by calling its shutdown method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "if 'server' in globals() and server.started:\n",
    "    server.should_exit = True\n",
    "    print(\"✓ Server shutdown requested. It will stop after handling current requests.\")\n",
    "    print(\"  Note: The server will also stop automatically when you restart the kernel.\")\n",
    "else:\n",
    "    print(\"Server is not running or has already stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. More Resources\n",
    "\n",
    "For more resources on developing AI Agents with Langchain, see:\n",
    "\n",
    "- [Langchain Documentation](https://docs.langchain.com/oss/python/langchain/overview) - The official Langchain documentation where all usage-related documentation can be found.\n",
    "- [Langchain Academy](https://academy.langchain.com/) - The official Langchain Academy provides extensive educational resources. The [Foundation Introduction to Langchain Python](https://academy.langchain.com/courses/foundation-introduction-to-langchain-python) course introduces the fundamentals of developing Agents with Langchain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
