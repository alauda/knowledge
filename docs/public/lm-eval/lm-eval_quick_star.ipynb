{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LM-Evaluation-Harness Quick Start Guide\n",
        "\n",
        "This notebook demonstrates how to use the lm-evaluation-harness (lm-eval) to evaluate language models using command-line interface.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Install lm-eval with the required backends:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install lm-eval with API support\n",
        "!pip install \"lm_eval[api]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Tasks\n",
        "\n",
        "First, let's see what evaluation tasks are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all available tasks (showing first 20 lines)\n",
        "!lm-eval ls tasks | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search for Specific Tasks\n",
        "\n",
        "You can search for specific tasks using grep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for MMLU tasks\n",
        "!lm-eval ls tasks | grep mmlu | head -20\n",
        "\n",
        "# Search for math-related tasks\n",
        "!lm-eval ls tasks | grep -i math\n",
        "\n",
        "# Search for Chinese language tasks\n",
        "!lm-eval ls tasks | grep zho"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quick Test with Limited Examples\n",
        "\n",
        "Before running a full evaluation, it's good practice to test with a small number of examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with 5 examples from hellaswag\n",
        "# Replace the base_url and model name with your local API endpoint\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-0.5B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks hellaswag \\\n",
        "    --limit 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluate on Multiple Tasks\n",
        "\n",
        "Run evaluation on multiple tasks suitable for API models (generation-based tasks):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on GSM8K (math reasoning)\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results \\\n",
        "    --log_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate with Configuration File\n",
        "\n",
        "For more complex evaluations, use a YAML configuration file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a configuration file\n",
        "config = \"\"\"\n",
        "model: local-chat-completions\n",
        "model_args:\n",
        "  model: Qwen/Qwen2.5-7B-Instruct\n",
        "  base_url: http://localhost:8000/v1\n",
        "tasks:\n",
        "  - gsm8k\n",
        "  - arc_easy\n",
        "  - hellaswag\n",
        "batch_size: 8\n",
        "output_path: ./results\n",
        "log_samples: true\n",
        "\"\"\"\n",
        "\n",
        "with open('eval_config.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "print(\"Configuration file created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation with config file\n",
        "!lm-eval --config eval_config.yaml --limit 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Evaluation Suite\n",
        "\n",
        "Run a comprehensive evaluation on multiple benchmarks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation (generation-based tasks for API models)\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k,arc_easy,arc_challenge,boolq,piqa \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./comprehensive_results \\\n",
        "    --log_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. View Results\n",
        "\n",
        "After evaluation completes, you can view the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load and display results\n",
        "with open('./results/results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Display the results\n",
        "print(\"=== Evaluation Results ===\")\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Explain common metrics\n",
        "print(\"\\n=== Common Output Metrics ===\")\n",
        "print(\"- acc: Accuracy (proportion of correct answers)\")\n",
        "print(\"- acc_norm: Normalized accuracy (using length-normalized probabilities)\")\n",
        "print(\"- exact_match: Exact string match between prediction and reference\")\n",
        "print(\"- pass@1, pass@10: Percentage of problems solved (for code generation)\")\n",
        "print(\"- f1: F1 score (harmonic mean of precision and recall)\")\n",
        "print(\"- bleu, rouge: Text similarity metrics for generation tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced: Task-Specific Examples\n",
        "\n",
        "### Mathematics Evaluation (GSM8K with Chain-of-Thought)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GSM8K with chain-of-thought reasoning\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k_cot \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/gsm8k_cot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multilingual Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on Chinese Belebele\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks belebele_zho_Hans \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/belebele_chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple MMLU Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on specific MMLU subjects\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks mmlu_abstract_algebra,mmlu_anatomy,mmlu_astronomy \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/mmlu_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Caching and Resume\n",
        "\n",
        "Use caching to resume interrupted evaluations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run with caching enabled\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k \\\n",
        "    --batch_size 8 \\\n",
        "    --use_cache ./cache \\\n",
        "    --output_path ./results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips and Best Practices\n",
        "\n",
        "1. **Always test first**: Use `--limit 5` or `--limit 10` to verify your setup before running full evaluations\n",
        "2. **Save results**: Use `--output_path` and `--log_samples` for reproducibility\n",
        "3. **Choose appropriate tasks**: Refer to the complete task list in the documentation for detailed task information\n",
        "4. **Monitor resources**: Large evaluations can take time; monitor with `htop` or `nvidia-smi`\n",
        "5. **Use caching**: Enable `--use_cache` for long evaluations that might be interrupted\n",
        "6. **Batch size**: Adjust `--batch_size` based on your API rate limits and model capacity\n",
        "7. **API configuration**: Ensure your local model service is running and accessible at the `base_url` you specify\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Complete Task Documentation**: See the main documentation for a comprehensive list of all evaluation tasks and their capabilities\n",
        "- **lm-eval Documentation**: https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs\n",
        "- **GitHub Repository**: https://github.com/EleutherAI/lm-evaluation-harness"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
