{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LM-Evaluation-Harness Quick Start Guide\n",
        "\n",
        "This notebook demonstrates how to use the lm-evaluation-harness (lm-eval) to evaluate language models using command-line interface.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Install lm-eval with the required backends:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install lm-eval with API support\n",
        "!pip install \"lm_eval[api]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Tasks\n",
        "\n",
        "First, let's see what evaluation tasks are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all available tasks (showing first 20 lines)\n",
        "!lm-eval ls tasks | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search for Specific Tasks\n",
        "\n",
        "You can search for specific tasks using grep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for MMLU tasks\n",
        "!lm-eval ls tasks | grep mmlu | head -20\n",
        "\n",
        "# Search for math-related tasks\n",
        "!lm-eval ls tasks | grep -i math\n",
        "\n",
        "# Search for Chinese language tasks\n",
        "!lm-eval ls tasks | grep zho"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set MODEL_NAME and BASE_URL\n",
        "\n",
        "Before running evaluations, set these to your API server:\n",
        "\n",
        "- **BASE_URL**: The full endpoint URL of your API.\n",
        "  - For **chat API**: `http://<host>:<port>/v1/chat/completions` (e.g. `http://localhost:8000/v1/chat/completions`).\n",
        "  - For **completions API**: `http://<host>:<port>/v1/completions` (e.g. `http://localhost:8000/v1/completions`).\n",
        "- **MODEL_NAME**: The model ID exposed by your server. Query the server (e.g. `GET http://localhost:8000/v1/models`) and use the `id` of the model you want to evaluate.\n",
        "\n",
        "Run the cell below to list models (optional). If your API is not on `localhost:8000`, set `BASE_URL_FOR_MODELS` in that cell and `BASE_URL` in the evaluation cells below to your server URL. Then set `MODEL_NAME` and `BASE_URL` in the evaluation cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: list models from your API (adjust the base URL if your server is not on localhost:8000)\n",
        "import urllib.request\n",
        "import json\n",
        "BASE_URL_FOR_MODELS = \"http://localhost:8000/v1\"  # without /chat/completions or /completions\n",
        "try:\n",
        "    with urllib.request.urlopen(f\"{BASE_URL_FOR_MODELS}/models\") as resp:\n",
        "        data = json.load(resp)\n",
        "    for m in data.get(\"data\", []):\n",
        "        print(m.get(\"id\", m))\n",
        "except Exception as e:\n",
        "    print(\"Could not list models (is your API server running?):\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Evaluation (chat API)\n",
        "\n",
        "Set `MODEL_NAME` and `BASE_URL` below. Use the **`--apply_chat_template`** flag so prompts are sent as chat messages and avoid \"messages as list[dict]\" / AssertionError. Then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Set these to your API server.\n",
        "export MODEL_NAME=\"your-model-id\"\n",
        "export BASE_URL=\"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "lm-eval --model local-chat-completions \\\n",
        "    --model_args model=$MODEL_NAME,base_url=$BASE_URL \\\n",
        "    --apply_chat_template \\\n",
        "    --tasks gsm8k \\\n",
        "    --batch_size 1 \\\n",
        "    --limit 10 \\\n",
        "    --output_path ./results \\\n",
        "    --log_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. View Results\n",
        "\n",
        "After evaluation completes, view the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists('./results/results.json'):\n",
        "    print(\"No results yet. Run 'Run Evaluation (chat API)' above first.\")\n",
        "else:\n",
        "    with open('./results/results.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(\"=== Evaluation Results ===\")\n",
        "    print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Example: local-completions with tokenizer (ARC)\n",
        "\n",
        "Tasks like ARC and MMLU use **loglikelihood** and require the **completions API** and a **tokenizer**. This demo uses **arc_easy** (small, single task) with `--limit 5` so it won't overload smaller models; for full MMLU use the same command with `--tasks mmlu` (see full doc). Set `MODEL_NAME`, `BASE_URL` (`/v1/completions`), and `TOKENIZER_PATH`, then run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Completions API: use /v1/completions. Tokenizer: required for loglikelihood; replace with your model's tokenizer (HuggingFace name or path).\n",
        "export MODEL_NAME=\"your-model-id\"\n",
        "export BASE_URL=\"http://localhost:8000/v1/completions\"\n",
        "export TOKENIZER_PATH=\"Qwen/Qwen2.5-7B\"   # replace according to your actual model\n",
        "\n",
        "lm-eval --model local-completions \\\n",
        "    --model_args model=$MODEL_NAME,base_url=$BASE_URL,tokenizer=$TOKENIZER_PATH \\\n",
        "    --tasks arc_easy \\\n",
        "    --batch_size 1 \\\n",
        "    --limit 5 \\\n",
        "    --output_path ./results_arc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Learn more\n",
        "\n",
        "For config file, multiple tasks, MMLU/completion API, and caching, see the documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips\n",
        "\n",
        "- Use `--batch_size 1` for API evaluation. Use model name from `GET /v1/models` and full `base_url` (e.g. `http://localhost:8000/v1/chat/completions`).\n",
        "- For more tasks, config file, MMLU/completion API, and caching, see the full documentation (*How to Evaluate LLM*) and [lm-eval docs](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
