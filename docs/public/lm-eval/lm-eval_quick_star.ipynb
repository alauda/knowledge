{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LM-Evaluation-Harness Quick Start Guide\n",
        "\n",
        "This notebook demonstrates how to use the lm-evaluation-harness (lm-eval) to evaluate language models using command-line interface.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Install lm-eval with the required backends:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install lm-eval with API support\n",
        "!pip install \"lm_eval[api]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List Available Tasks\n",
        "\n",
        "First, let's see what evaluation tasks are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all available tasks (showing first 20 lines)\n",
        "!lm-eval ls tasks | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search for Specific Tasks\n",
        "\n",
        "You can search for specific tasks using grep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for MMLU tasks\n",
        "!lm-eval ls tasks | grep mmlu | head -20\n",
        "\n",
        "# Search for math-related tasks\n",
        "!lm-eval ls tasks | grep -i math\n",
        "\n",
        "# Search for Chinese language tasks\n",
        "!lm-eval ls tasks | grep zho"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quick Test with Limited Examples\n",
        "\n",
        "Before running a full evaluation, it's good practice to test with a small number of examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with 5 examples from hellaswag\n",
        "# Replace the base_url and model name with your local API endpoint\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-0.5B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks hellaswag \\\n",
        "    --limit 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluate on Multiple Tasks\n",
        "\n",
        "Run evaluation on multiple tasks suitable for API models (generation-based tasks):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on GSM8K (math reasoning)\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results \\\n",
        "    --log_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate with Configuration File\n",
        "\n",
        "For more complex evaluations, use a YAML configuration file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a configuration file\n",
        "config = \"\"\"\n",
        "model: local-chat-completions\n",
        "model_args:\n",
        "  model: Qwen/Qwen2.5-7B-Instruct\n",
        "  base_url: http://localhost:8000/v1\n",
        "tasks:\n",
        "  - gsm8k\n",
        "  - arc_easy\n",
        "  - hellaswag\n",
        "batch_size: 8\n",
        "output_path: ./results\n",
        "log_samples: true\n",
        "\"\"\"\n",
        "\n",
        "with open('eval_config.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "print(\"Configuration file created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation with config file\n",
        "!lm-eval --config eval_config.yaml --limit 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. OpenAI API Evaluation\n",
        "\n",
        "To evaluate models via OpenAI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n",
        "\n",
        "# Evaluate GPT-4o-mini\n",
        "!lm-eval --model openai-chat-completions \\\n",
        "    --model_args model=gpt-4o-mini \\\n",
        "    --tasks gsm8k \\\n",
        "    --limit 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Evaluation Suite\n",
        "\n",
        "Run a comprehensive evaluation on multiple benchmarks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation (generation-based tasks for API models)\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k,arc_easy,arc_challenge,boolq,piqa \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./comprehensive_results \\\n",
        "    --log_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. View Results\n",
        "\n",
        "After evaluation completes, you can view the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load and display results\n",
        "with open('./results/results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# Display the results\n",
        "print(\"=== Evaluation Results ===\")\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Explain common metrics\n",
        "print(\"\\n=== Common Output Metrics ===\")\n",
        "print(\"- acc: Accuracy (proportion of correct answers)\")\n",
        "print(\"- acc_norm: Normalized accuracy (using length-normalized probabilities)\")\n",
        "print(\"- exact_match: Exact string match between prediction and reference\")\n",
        "print(\"- pass@1, pass@10: Percentage of problems solved (for code generation)\")\n",
        "print(\"- f1: F1 score (harmonic mean of precision and recall)\")\n",
        "print(\"- bleu, rouge: Text similarity metrics for generation tasks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Advanced: Task-Specific Examples\n",
        "\n",
        "### Mathematics Evaluation (GSM8K with Chain-of-Thought)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GSM8K with chain-of-thought reasoning\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k_cot \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/gsm8k_cot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multilingual Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on Chinese Belebele\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks belebele_zho_Hans \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/belebele_chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple MMLU Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on specific MMLU subjects\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks mmlu_abstract_algebra,mmlu_anatomy,mmlu_astronomy \\\n",
        "    --batch_size 8 \\\n",
        "    --output_path ./results/mmlu_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Caching and Resume\n",
        "\n",
        "Use caching to resume interrupted evaluations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run with caching enabled\n",
        "!lm-eval --model local-chat-completions \\\n",
        "    --model_args model=Qwen/Qwen2.5-7B-Instruct,base_url=http://localhost:8000/v1 \\\n",
        "    --tasks gsm8k \\\n",
        "    --batch_size 8 \\\n",
        "    --use_cache ./cache \\\n",
        "    --output_path ./results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips and Best Practices\n",
        "\n",
        "1. **Always test first**: Use `--limit 5` or `--limit 10` to verify your setup\n",
        "2. **Save results**: Use `--output_path` and `--log_samples` for reproducibility\n",
        "3. **Choose appropriate tasks**: \n",
        "   - Use generation tasks (`gsm8k`, `arc_easy`, etc.) for API models without logprobs\n",
        "   - Use all task types for local models that provide logprobs\n",
        "4. **Monitor resources**: Large evaluations can take time; monitor with `htop` or `nvidia-smi`\n",
        "5. **Use caching**: Enable `--use_cache` for long evaluations that might be interrupted\n",
        "6. **Batch size**: Adjust based on your API rate limits and model capacity\n",
        "\n",
        "## Common Task Categories\n",
        "\n",
        "### Generation Tasks (Work with all API types)\n",
        "- `gsm8k`, `gsm8k_cot` - Math reasoning\n",
        "- `humaneval`, `mbpp` - Code generation\n",
        "- `truthfulqa_gen` - Truthfulness (generation variant)\n",
        "\n",
        "### Multiple Choice Tasks (May work without logprobs)\n",
        "- `mmlu` - General knowledge (57 subjects)\n",
        "- `arc_easy`, `arc_challenge` - Science questions\n",
        "- `hellaswag` - Commonsense reasoning\n",
        "- `winogrande` - Pronoun resolution\n",
        "- `boolq` - Yes/no questions\n",
        "- `piqa` - Physical commonsense\n",
        "\n",
        "### Loglikelihood Tasks (Require logprobs - local models only)\n",
        "- `lambada_openai` - Word prediction\n",
        "- Perplexity evaluation tasks\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Documentation**: https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs\n",
        "- **GitHub**: https://github.com/EleutherAI/lm-evaluation-harness\n",
        "- **Open LLM Leaderboard**: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
